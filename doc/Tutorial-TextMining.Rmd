---
title: "Title"
output: html_notebook
    toc: true
    toc_depth: 2
---

The inauguration speeches are always one of the most important events of the presidents, and they contain lots of messages and information. In this project, we will focus on the inauguration speeches of the US presidents ** who were re-elected (successfully)** by applying Natural Language Processing (NLP) methods.

\textbf{I. Overview of the data}

The President of the United States is the head of state and head of government of the United States of America and the term of office for the president is four years. Franklin D. Roosevelt, as we known, was elected to a 3rd and a 4th term, and died in his president office. In response to Roosevelt being elected to four terms, the Twenty-second Amendment was adopted in 1951. The amendment bars anyone from being elected president more than twice term, and since then, many presidents chose to re-elect in the 2nd term and quit before the 3rd term (if they get re-elected).

In the project, We only choose all inauguration speeches of every president. 58 speeches are considered. We usually consider 16 getting re-elected presidents and thus 32 speeches (16 from their 1st term speech, and other 16 from 2nd's).



Before we make a text analysis, we should do some preparation first.

\textit{# Step 0: check and install needed packages. Load the libraries and functions.} 

```{r, message=FALSE, warning=FALSE, echo = FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
               "RANN", "tm", "topicmodels")
# rvest: scrap data online
# tibble: deal with table data
# qdap: quantitive method esp. text mining
# sentimentr: sentiment
# gplots: plot
# dplyr: pre-process, apply
# tm: text-mining

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
install.packages("rjava")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

\textit{# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.\\Following the example of [Jerid Francom](https://francojc.github.io/2015/03/01/web-scraping-with-rvest-in-r/), we used [Selectorgadget](http://selectorgadget.com/) to choose the links we would like to scrap. For this project, we selected all inaugural addresses of past presidents, nomination speeches of major party candidates and farewell addresses. We also included several public speeches from Donald Trump for our textual analysis of presidential speeches. }

```{r, message=FALSE, warning=FALSE, echo = FALSE}
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
#head(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.

```

\textit{# Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap.}

```{r, echo = FALSE}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
```

\textit{We assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts.}

\textit{# Step 3: scrap the texts of speeches from the speech URLs.}

```{r, echo = FALSE}
speech.list=inaug.list
speech.list$type=c(rep("inaug", nrow(inaug.list)))
speech.url=inaug
speech.list=cbind(speech.list, speech.url)
```

\textit{Based on the list of speeches, we scrap the main text part of the transcript's html page. For simple html pages of this kind, [Selectorgadget](http://selectorgadget.com/) is very convenient for identifying the html node that `rvest` can use to scrap its content. For reproducibility, we also save our scrapped speeches into our local folder as individual speech files. }

```{r, echo= FALSE}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/InauguralSpeeches/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```

\textit{# Step 4: data Processing --- generate list of sentences \\ We will use sentences as units of analysis for this project, as sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, we apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."\\We assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).}

```{r, message=FALSE, warning=FALSE, echo = FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of-sentence marks. 
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 

```

# Step 5: Data analysis --- length of sentences


## Overview of sentence length distribution by different types of speeches. 

### Nomination speeches 

First, we look at *nomination acceptance speeches* at major party's national conventions. For relevant to Trump's speeches, we limit our attention to speeches for the first terms of former U.S. presidents.  We noticed that a number of presidents have very short sentences in their nomination acceptance speeches. 

#### First term

find the right president
```{r, fig.width = 3, fig.height = 2}

par(mar=c(4, 9, 2, 2))

sentence.list.inaug=filter(sentence.list, Term==1)

sentence.list.inaug$File=factor(sentence.list.inaug$File)

sentence.list.inaug2=filter(sentence.list, Term==2, File%in% sentence.list.inaug$File)

sentence.list.inaug=filter(sentence.list, Term==1, File%in%sentence.list.inaug2$File)


sentence.list.inaug$FileOrdered=reorder(sentence.list.inaug$File, 
                                  sentence.list.inaug$word.count, 
                                  mean, 
                                  order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list.inaug,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.45, cex.axis=0.7, cex.lab=0.9,
         spacing=8/nlevels(sentence.list.inaug$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Inauguration speeches, 1st term")

```

#### Second term

```{r, fig.width = 3, fig.height = 2}

par(mar=c(4, 9, 2, 2))
Comp <- function(x)
{
  Names <- c("GeorgeWashington", "AndrewJackson", "JamesMadison", "WilliamMcKinley", "ThomasJefferson", "UlyssesSGrant", "JamesMonroe", "AbrahamLincoln", "WoodrowWilson", "BarackObama", "DwightDEisenhower", "FranklinDRoosevelt", "RichardNixon", "RonaldReagan", "WilliamJClinton", "GeorgeWBush")
  for (i in 1:16)
    if (x == Names[i])
      return(i)
}

####inaug 2nd terms
#sentence.list.inaug2=filter(sentence.list, 
#                        type=="inaug", Term==2, File %in% sentence.list.inaug$File)
for (i in 1:length(sentence.list.inaug2$File))
{
  sentence.list.inaug2$File[i] <- paste(sentence.list.inaug2$File[i], ",", Comp(sentence.list.inaug2$File[i]))
}

sentence.list.inaug2$File=factor(sentence.list.inaug2$File)

sentence.list.inaug2$FileOrdered=reorder(sentence.list.inaug2$File, 
                                  sentence.list.inaug2$word.count, 
                                  mean, 
                                  order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list.inaug2,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.7, cex.lab=0.9,
         spacing=3/nlevels(sentence.list.inaug2$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Inaugration speeches, 2nd term")
```

What are these short sentences?
```{r}
Search_Words <- function(Word, Name, Terms)
{
  Str <- as.matrix(sentence.list%>%filter(File == Name, Term == Terms) 
                   %>% select(sentences))
  return(Str[grep(Word, Str)])
}
## Search "Word" From President's "Name"

Search_Words("gay", "BarackObama", 2)

Search_Words("tyranny", "GeorgeWBush", 2)

Search_Words("war", "AbrahamLincoln", 2)

Search_Words("peace", "FranklinDRoosevelt", 4)


```


# Step 5: Data analysis --- sentiment analsis

## Sentence length variation over the course of the speech, with emotions. 

How our presidents (or candidates) alternate between long and short sentences and how they shift between different sentiments in their speeches. It is interesting to note that some presidential candidates' speech are more colorful than others. Here we used the same color theme as in the movie "Inside Out."

![image](http://www.staffordschools.net/cms/lib011/VA01818723/Centricity/Domain/3574/character_icon.png)

```{r, fig.height=2.5, fig.width=2}
par(mfrow=c(4,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)

f.plotsent.len(In.list=sentence.list, InFile="RonaldReagan", InType = "inaug", 
                InTerm=1, President="Ronald Reagan-1st")

f.plotsent.len(In.list=sentence.list, InFile="RonaldReagan", InType = "inaug", 
                InTerm=2, President="Ronald Reagan-2nd")

f.plotsent.len(In.list=sentence.list, InFile="BarackObama", InType = "inaug", 
                InTerm=1, President="Barack Obama-1st")

f.plotsent.len(In.list=sentence.list, InFile="BarackObama", InType = "inaug", 
                InTerm=2, President="Barack Obama-2nd")
```

### What are the emotionally charged sentences?

```{r}
print("Donald Trump")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="DonaldJTrump", type=="inaug", Term==1, word.count>=5)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
paste(names(speech.df[,-1]), ":", as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)]))

```


## Clustering of emotions
```{r, fig.width=4, fig.height=2}
par(mar=c(4, 6, 2, 1), mfrow=c(1,2))

emo.means=colMeans(select(sentence.list%>%filter(Term==1), anger:trust)>0.01)
col.use=c("red2", "palegreen1", 
            "burlywood4", "dodgerblue4",
            "plum1", "darkgreen", 
            "lightgoldenrod1", "ivory")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches-1st Term")

emo.means=colMeans(select(sentence.list%>%filter(Term==2), anger:trust)>0.01)
col.use=c("red2", "palegreen1", 
            "burlywood4", "dodgerblue4",
            "plum1", "darkgreen", 
            "lightgoldenrod1", "ivory")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches-2nd Term")
```

```{r, fig.width=2, fig.height=2}
heatmap.2(cor(sentence.list%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
```

```{r, fig.height=3.3, fig.width=3.7}
presid.summary=tbl_df(sentence.list)%>%
  #filter(File%in%sentence.list.inaug$File)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)

rownames(presid.summary)=as.character((presid.summary[,1]))

km.res=kmeans(presid.summary[,-1], iter.max=200, 2)

fviz_cluster(km.res, 
             stand=F, repel= TRUE, ellipse.type = "t",
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE, pointsize =2)


presid.summary$party <- rep(NA, nrow(presid.summary))
for (i in seq(nrow(presid.summary)))
{
  if (sum(table(sentence.list$Party[sentence.list$File == presid.summary[i,1]]))==0)
    presid.summary$party[i] <- "Else"
  else
    presid.summary$party[i] <- as.character(names(table(sentence.list$Party[sentence.list$File == presid.summary[i,1]])))
  if (presid.summary$party[i] != "Democratic" & presid.summary$party[i] !="Republican")
    presid.summary$party[i] <- "Else"
}
presid.summary$party <- factor(presid.summary$party)



for (i in seq(nrow(presid.summary)))
{
  presid.summary$x[i] <- sqrt(sum(presid.summary[i, c("anger", "disgust", "fear", "sadness")] ^2))
  presid.summary$y[i] <- sqrt(sum(presid.summary[i, c("anticipation", "joy", "surprise", "trust")] ^2))
}
central.1x <- sqrt(sum(km.res$centers[1, c("anger", "disgust", "fear", "sadness")] ^2))
central.2x <- sqrt(sum(km.res$centers[2, c("anger", "disgust", "fear", "sadness")] ^2))
central.1y <- sqrt(sum(km.res$centers[1, c("anticipation", "joy", "surprise", "trust")] ^2))
central.2y <- sqrt(sum(km.res$centers[2, c("anticipation", "joy", "surprise", "trust")] ^2))



ggplot(data = presid.summary, aes(x, y, group = party, colour = party, size = factor(km.res$cluster), label = rownames(presid.summary)))  + geom_text(check_overlap = TRUE)
```

```{r, fig.height=3.3, fig.width=3.7}
presid.summary=tbl_df(sentence.list)%>%
  filter(Term == 1, File%in%sentence.list.inaug$File)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust),
    term = 1
    #negative=mean(negative),
    #positive=mean(positive)
  )
presid.summary2=tbl_df(sentence.list)%>%
  filter(Term == 2, File%in%sentence.list.inaug$File)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust),
    term = 2
    #negative=mean(negative),
    #positive=mean(positive)
  )
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary) <- paste(as.character((presid.summary[,1])),"1",sep="")
presid.summary2=as.data.frame(presid.summary2)
rownames(presid.summary2) <- paste(as.character((presid.summary2[,1])),"2",sep="")
presid.summary <- rbind(presid.summary, presid.summary2)


km.res=kmeans(presid.summary[,2:9], iter.max=200,
              2)
fviz_cluster(km.res, 
             stand=F, repel= TRUE, ellipse.type = "convex",
             data = presid.summary[,2:9], xlab="", xaxt="n", ylab="", main="",
             show.clust.cent=FALSE)
```
# Step 5: Data analysis --- Topic modeling

For topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. 

```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text mining
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

### Topic modeling

Gengerate document-term matrices. 

```{r}
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
colTotals <- apply(dtm, 2, sum)

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```

Run LDA

```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../figs/LDAGibbs",k,"DocsToTopics.csv"))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,10))
write.csv(ldaOut.terms,file=paste("../figs/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../figs/LDAGibbs",k,"TopicProbabilities.csv"))
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```

Based on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic. This part require manual setup as the topics are likely to change. 

```{r}
topics.hash=c("Election",	"Reform","Power","Economy","Life","Willing","Belief","Duty","Time","Freedom","Right","Patriotism","Liberty","Public","Peace")

corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

## Clustering of topics
```{r, fig.width=3.3, fig.height=5.2}
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
              filter(Term == 1, File%in%sentence.list.inaug$File)%>%
              select(File, Election:Peace)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary) <- paste(as.character((topic.summary[,1])),"1",sep="")

topic.summary2=tbl_df(corpus.list.df)%>%
              filter(Term == 2, File%in%sentence.list.inaug$File)%>%
              select(File, Election:Peace)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
topic.summary2=as.data.frame(topic.summary2)
rownames(topic.summary2) <- paste(as.character((topic.summary2[,1])),"2",sep="")

topic.summary <- rbind(topic.summary, topic.summary2)
# [1] "Election"   "Reform"     "Power"      "Economy"    "Life"      
# [6] "Willing"    "Belief"     "Duty"       "Time"       "Freedom"   
#[11] "Right"      "Patriotism" "Liberty"    "Public"     "Peace"      
 


#topic.plot=c(1, 13, 9, 11, 8, 3, 7)
topic.plot <- 1:15
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
```

```{r, fig.width=3.3, fig.height=2}
# [1] "Election"   "Reform"     "Power"      "Economy"    "Life"      
# [6] "Willing"    "Belief"     "Duty"       "Time"       "Freedom"   
#[11] "Right"      "Patriotism" "Liberty"    "Public"     "Peace"      
 

par(mfrow=c(2, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")

topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])

speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeWBush",Term==1)%>%
  select(sent.id, Election:Peace)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1], 
             xlab="Sentences", ylab="Topic share", main="George W Bush, Inauguration")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="BarackObama", Term==1)%>%
  select(sent.id, Election:Peace)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Barack Obama, Inauguration")

```


```{r}
speech.df=tbl_df(corpus.list.df)%>%filter( word.count<20)%>%select(sentences, Election:Peace)

as.character(speech.df$sentences[apply(as.data.frame(speech.df[,-1]), 2, which.max)])

names(speech.df)[-1]

```


```{r, fig.width=3, fig.height=3}
presid.summary=tbl_df(corpus.list.df)%>%
  filter(type=="inaug", File%in%sentence.list.inaug$File)%>%
  select(File, Election:Peace)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=tbl_df(corpus.list.df)%>%
              filter(Term == 1, File%in%sentence.list.inaug$File)%>%
              select(File, Election:Peace)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary) <- paste(as.character((presid.summary[,1])),"1",sep="")

presid.summary2=tbl_df(corpus.list.df)%>%
              filter(Term == 2, File%in%sentence.list.inaug$File)%>%
              select(File, Election:Peace)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
presid.summary2=as.data.frame(presid.summary2)
rownames(presid.summary2) <- paste(as.character((presid.summary2[,1])),"2",sep="")

presid.summary <- rbind(presid.summary, presid.summary2)

km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
              2)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,ellipse.type = "confidence",
             data = presid.summary[,-1],
             show.clust.cent=FALSE, labelsize = 8, xlab="",ylab="",main="")
```

# Readings for NLP with Python

+ [Natural Language Processing with Python](http://www.nltk.org/book/)
+ [A shorter tutorial](https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk)
+ [Sentiment analysis](https://pythonspot.com/en/python-sentiment-analysis/)
+ [Topic modeling](https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730)
